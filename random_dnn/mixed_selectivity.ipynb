{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23c7f302",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221c8af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import RMT\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54fc957",
   "metadata": {},
   "outputs": [],
   "source": [
    "torchvision.datasets.CIFAR100(root=\"fig/datasets\", download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cf2c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = torchvision.datasets.CIFAR10(root=\"fig/datasets\", transform=torchvision.transforms.ToTensor())\n",
    "dataset_data = np.array(dataset.data, dtype=np.float32)\n",
    "dataset_targets = np.array(dataset.targets)\n",
    "display(dataset.data.shape)\n",
    "class_to_data = {name: dataset_data[dataset_targets == l] for name, l in dataset.class_to_idx.items()}\n",
    "# show number of images (and tensor shape) per class\n",
    "display({k: v.shape for k, v in class_to_data.items()})\n",
    "layer_size = np.prod(dataset.data.shape[1:])\n",
    "display(layer_size)\n",
    "norm_data_dict = {\n",
    "    k: (v / np.mean(v**2, tuple(range(1, len(v.shape))), keepdims=True)**0.5)\n",
    "    for k, v in class_to_data.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d45bfd",
   "metadata": {},
   "source": [
    "## Demonstration on empirical MLPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28b011d",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_dict = {}\n",
    "\n",
    "alpha = 1.5\n",
    "sigma_W = 1.5\n",
    "\n",
    "fp_norm = (\n",
    "    RMT.MFT_map(RMT.q_star_MC(alpha, sigma_W)[-1], alpha, sigma_W, usetqdm=False)[\n",
    "        \"postact_sq_mean\"\n",
    "    ][0]\n",
    "    ** 0.5\n",
    ")\n",
    "\n",
    "for label, data in tqdm(norm_data_dict.items()):\n",
    "    num_items = 2 # len(data)\n",
    "    # flatten the images and scale by fixed point norm\n",
    "    x0 = data[:num_items].reshape(num_items, -1) * fp_norm\n",
    "    xs = RMT.MLP(\n",
    "        torch.tensor(x0),\n",
    "        10,\n",
    "        alpha,\n",
    "        sigma_W,\n",
    "        seed=42,\n",
    "        fast=True,\n",
    "        usetqdm=False,\n",
    "    )[\"postact\"]\n",
    "    norms = (np.array([x0, *xs]) ** 2).mean(-1) ** 0.5\n",
    "\n",
    "    outputs_dict[label] = norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fb9ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "color_dict = {label: plt.cm.tab10(i) for i, label in enumerate(norm_data_dict.keys())}\n",
    "for label, norms in outputs_dict.items():\n",
    "    plt.plot(\n",
    "        norms.mean(-1),\n",
    "        \"-o\",\n",
    "        # alpha=x0.shape[0] ** -0.5,\n",
    "        color=color_dict[label],\n",
    "        label=label,\n",
    "    )\n",
    "    plt.fill_between(\n",
    "        np.arange(len(norms)),\n",
    "        np.quantile(norms, 0.01, axis=-1),\n",
    "        np.quantile(norms, 0.99, axis=-1),\n",
    "        color=color_dict[label],\n",
    "        alpha=0.5,\n",
    "    )\n",
    "\n",
    "plt.ylim([0.25, 0.75])\n",
    "# plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0436b37",
   "metadata": {},
   "source": [
    "## MFT demonstration\n",
    "takes longer than the MLP (about 7m33s on a Tesla M4 for CIFAR-10, 10 layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4e3841",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_dict = {}\n",
    "\n",
    "alpha = 1.5\n",
    "sigma_W = 1.5\n",
    "\n",
    "fp_norm = (\n",
    "    RMT.MFT_map(RMT.q_star_MC(alpha, sigma_W)[-1], alpha, sigma_W, usetqdm=False)[\n",
    "        \"postact_sq_mean\"\n",
    "    ][0]\n",
    "    ** 0.5\n",
    ")\n",
    "\n",
    "# gpu memory bottleneck due to `RMT.MFT_map::postact_samples`\n",
    "max_num_items_in_batch = 100\n",
    "\n",
    "for label, data in tqdm(norm_data_dict.items()):\n",
    "    num_items = 100  # len(data)\n",
    "    x0 = data[:num_items].reshape(num_items, -1) * fp_norm\n",
    "    q0 = sigma_W**alpha * (abs(x0) ** alpha).mean(-1)\n",
    "    MFT_maps = []\n",
    "    for q0_chunk in tqdm(\n",
    "        np.array_split(q0, max(1, num_items // max_num_items_in_batch), axis=0),\n",
    "        leave=False,\n",
    "    ):\n",
    "        torch.manual_seed(42)\n",
    "        val = RMT.MFT_map(\n",
    "            q0_chunk,\n",
    "            alpha,\n",
    "            sigma_W,\n",
    "            num_layers=10,\n",
    "            agg_postact=lambda x: {\n",
    "                \"sq_mean\": (x**2).mean(-1),\n",
    "                \"alpha_mean\": (abs(x) ** alpha).mean(-1),\n",
    "            },\n",
    "            usetqdm=False,\n",
    "        )\n",
    "        MFT_maps.append(val)\n",
    "    # average over Monte-Carlo realisations (axis=2), then concatenate chunks of inputs (axis=1)\n",
    "    agg_stats = {\n",
    "        k: np.concatenate([np.mean(mft[k], axis=2) for mft in MFT_maps], axis=1)\n",
    "        for k in MFT_maps[0]\n",
    "    }\n",
    "    norms = np.array([(x0**2).mean(-1), *agg_stats[\"postact_sq_mean\"]]) ** 0.5\n",
    "    alpha_norms = np.array(\n",
    "        [(abs(x0) ** alpha).mean(-1), *agg_stats[\"postact_alpha_mean\"]]\n",
    "    ) ** (1 / alpha)\n",
    "    qs = agg_stats[\"q_val\"]\n",
    "\n",
    "    outputs_dict[label] = norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a444e43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.savez('fig/mixed_selectivity/outputs_MLP.npz', **outputs_dict)\n",
    "outputs_dict = np.load('fig/mixed_selectivity/outputs_MLP.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda825a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "color_dict = {label: plt.cm.tab10(i) for i, label in enumerate(norm_data_dict.keys())}\n",
    "for label, norms in outputs_dict.items():\n",
    "    ax.plot(\n",
    "        norms.mean(-1),\n",
    "        \"-o\",\n",
    "        # alpha=x0.shape[0] ** -0.5,\n",
    "        color=color_dict[label],\n",
    "        label=label,\n",
    "    )\n",
    "    ax.fill_between(\n",
    "        np.arange(len(norms)),\n",
    "        np.quantile(norms, 0.01, axis=-1),\n",
    "        np.quantile(norms, 0.99, axis=-1),\n",
    "        color=color_dict[label],\n",
    "        alpha=0.5,\n",
    "    )\n",
    "\n",
    "ax.set(\n",
    "    xlabel='layer',\n",
    "    ylabel='postact_norm',\n",
    "    # ylim=(0.25, 0.75),\n",
    ")\n",
    "# ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3317e9d5",
   "metadata": {},
   "source": [
    "# Testing the agg stats function for the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab158c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mixed_selectivity import MFT_map\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.set_default_device(\"cpu\")\n",
    "result = MFT_map(\n",
    "    dataset_name=\"CIFAR10\",\n",
    "    alpha=1.5,\n",
    "    sigma_W=1.5,\n",
    "    sigma_b=0.0,\n",
    "    num_layers=10,\n",
    "    chunk_size=100,\n",
    "    seed=42,\n",
    "    num_inputs=550,\n",
    ")\n",
    "\n",
    "print({k: v.shape for k, v in result.items()})\n",
    "\n",
    "vals = result[\"postact_sq_mean\"]\n",
    "plt.plot(range(vals.shape[0]), vals.mean(axis=1), \"-o\")\n",
    "plt.fill_between(\n",
    "    range(vals.shape[0]),\n",
    "    np.quantile(vals, 0.1, axis=1),\n",
    "    np.quantile(vals, 0.9, axis=1),\n",
    "    alpha=0.5,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
